<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <style>
      table,
      th,
      td {
        border: 1px solid white;
        border-collapse: collapse;
        width: 700px;
      }
      th,
      td {
        background-color: #96d4d4;
      }
    </style>
  </head>
  <body>
    <h1>This is Task 1</h1>
    <br />
    <table>
      <tr>
        <th>Firstname</th>
        <th>Lastname</th>
        <th>Age</th>
      </tr>
      <tr>
        <td>Jill</td>
        <td>Smith</td>
        <td>50 </td>
      </tr>
      <tr>
        <td>Eve</td>
        <td>Jackson</td>
        <td>94</td>
      </tr>
      <tr>
        <td>John</td>
        <td>Doe</td>
        <td>80</td>
      </tr>
    </table>
  </body>
</html>
from flask import Flask, request, jsonify
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

app = Flask(__name__)

@app.route('/preprocess', methods=['POST'])
def preprocess():
    # Get the file path from the request
    file_path = request.form.get('file_path')
    
    if not file_path:
        return jsonify({'error': 'No file path provided'}), 400
    
    # Load the dataset
    try:
        data = pd.read_csv(file_path)
    except Exception as e:
        return jsonify({'error': str(e)}), 400
    
    # Separate features and target if target is specified
    if 'target' in request.form:
        target_column = request.form['target']
        if target_column not in data.columns:
            return jsonify({'error': f'Target column "{target_column}" not found in dataset'}), 400
        X = data.drop(target_column, axis=1)
        y = data[target_column]
    else:
        X = data
    
    # Identify categorical and numerical columns
    categorical_cols = X.select_dtypes(include=['object']).columns
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns

    # Define preprocessing for numerical data
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    # Define preprocessing for categorical data
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Bundle preprocessing for numerical and categorical data
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])

    # Apply preprocessing
    X_preprocessed = preprocessor.fit_transform(X)

    # Convert the preprocessed data back to a DataFrame for easier inspection
    # Get column names for the preprocessed data
    onehot_columns = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)
    all_columns = np.append(numerical_cols, onehot_columns)
    X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=all_columns)

    # If target column was specified, add it back to the DataFrame
    if 'target' in request.form:
        X_preprocessed_df[target_column] = y.values

    # Return the preprocessed data as JSON
    return X_preprocessed_df.to_json(orient='split')

if __name__ == '__main__':
    app.run(debug=True)
